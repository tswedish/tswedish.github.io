---
layout: post
title: "Deep Learning Alchemy: Part 2 - Going Deeper with Image Alignment"
date: 2018-07-28
---

*Note [4/18/2019]: I haven't posted this on my live site until now, but this post is an example of things I was thinking about last summer while at my internship at Skydio. I never made it public because there were some additional things I wanted to add but... life got in the way and I stopped updating my blog. Last fall I listened in on Luca Carlone's Visual Navigation class, and I learned much more about some of the SOTA ways of addressing this problem, particularly using the Lie Algebra associated with SE(3) to calculate the Jacobian of the orientation. I haven't been posting things since, so I decided to put this up at least to motivate future posts!*

This post is the second part of a previous post. I have a backlog of projects
I'd like to start posting to this site, but until now I haven't decided how I
should scope them. I've decided to just post whatever I feel like is ready, and
give up on any sort of illusions that I have focused interests.

Since Part 1, I'm excited by some recent work that leverages
geometrically constrained photometric alignment and deep learning together. In
particualar I really enjoy [this paper](https://arxiv.org/pdf/1802.05522.pdf),
that learns motion and depth estimates by using image warping and geometric
constraints from point cloud alignment.  There are a number of papers published
before, such as [this
one](https://people.eecs.berkeley.edu/~tinghuiz/projects/SfMLearner/cvpr17_sfm_final.pdf)
that propose many of the things I would call "deep learning alchemy." It's an
exciting time!

In this post, I'd like to take another stab at the math for differentiable
image warping as well as compare photometric and feature point based methods.
In Part 1 we used PyTorch to do the heavy lifting for us, which I think is a
pretty awesome way to use deep learning frameworks, but in practice this method
was just too slow. While it's certainly possible to get decent performance by
being careful how the backwards path is calculated and saved in memory for
subsequent calls, why don't we just solve the analytical problem?

While in the previous post we stared blankly at mathematical expressions we
didn't know how to differentiate, in this post we will use a symbolic math
library! When I first was thinking symbolic solvers, I immediately thought to
write this post using [Mathematica](https://www.wolfram.com/mathematica/), a
great language and programming environment for symbolic expression
manipulation, but everything changed once I tried
[Sympy](http://www.sympy.org/en/index.html). Sympy brings symbolic expression
programming to python and supports some really powerful code generation tools.
What I really like about sympy is the speed one can go from math to extremely
fast code! There's another library I should probably mention that gave me
goosebumps (although we won't use it here) [CVXPY](http://www.cvxpy.org/) which
is a really great abstraction for solving convex optimization problems, and is
apparently used by SpaceX for the model-predictive control, which I think is
pretty sweet (well I guess it's [CVXGEN](https://cvxgen.com/docs/index.html) that gets credit, but close enough! See [here](https://www.nae.edu/19582/Bridge/164237/164334.aspx)).

## The Wonderful World of Math Oriented Languages

If there's any lesson to be taken from this series, it's that there are an
emerging set of libraries that are math oriented rather than computationally
oriented, and it's making quite a few engineering problems easy and practical
to solve in environments like python with surprisingly little code. These math
oriented languages typically pick an abstraction that is pretty in line with
the underlying math, providing a set of primitives that when composed allow you
to model the problem. When you're ready to solve, you don't need to do
much, the computer will do much of the busy work for you.  It's 2018, we
shouldn't be afraid of a few derivatives!

There's another piece here that's interesting. The exploding popularity of deep
learning frameworks are combining a few useful things. They are math oriented,
which makes modelling problems easier, they also have a way to implement
back-prop to solve hard optimization problems, and they also typically are
performance minded making efficient use of GPUs without adding programming
complexity. This means that you can express things in a high-level way, and the
framework will still run efficiently, even in settings that seemed quite
difficult, such as GPUs on [mobile phones](https://js.tensorflow.org/).

So what does this all mean? I think these trends indicate that learning to
model these problems well in high-level terms will lead to solutions that are
easier to understand, easier to maintain, fast, and more scalable. As humans,
we should focus on the modeling, and making our math efficient (choosing good
approximation, for example) to make the most impact. This is in contrast to
spending more effort on low level optimizations (at least from the point of
view of a system designer!).

## Let's Calculate our Jacobians Properly with Sympy

So enough conceptual waxing, let's perform some Jacobian witchcraft. Ok, well,
let's at least use Sympy. In part 1 we wrote out our forward model for our
image warping problem. Here it is again for reference:

$$
\begin{bmatrix} u \\ v \end{bmatrix} = W(x,\theta) = \pi(K(R_{\theta}x + t_{\theta}))
$$

Given 6 numbers "$$\theta$$" (3 are used to create $$R$$ and 3 for $$t$$), we
can fully specify how points in 3D will appear in our camera for a given 6-DOF
transformation. That's easy enough, now we want to figure out given desired way
that those 3D points appear, how will those 6 numbers change? This is captured
by the Jacobian, which given a vector in the image plane $$[u, v]$$, produces a
$$6 \times 2$$ matrix that describes how a change in $$[u, v]$$ produces a
change in our 6 parameters.  

Our Jacobian is simply the derivative (or rate of change) of the output of our
warping and projection function $$W$$ with respect to our 6 parameters.

In sympy, we just need to create our forward warp and then ask it to tell us
the derivative with respect to each of our 6 warping parameters. The generated
output is still symbolic, but gives us an exact expression for the Jacobian.
This we can simplify using approximations, and convert into code for silly fast
operation (see below).

The first thing we do is define our symbolic variable (p1-p6 for elements in
$$\theta$$ and x1-x3 for our points), making sure to tell Sympy we know these
values will be real. Next we express the Rodrigues Formula in terms of p1-p3 so
that we get our rotation matrix $$R$$. Using the definition from Part 1:

{% highlight python %}
from sympy import *
p1,p2,p3 = symbols('p1 p2 p3', real=True)

r = Matrix([[p1],[p2],[p3]])

theta = r.norm()
rhat = r / theta

R1 = cos(theta)*eye(3)
R2 = (1 - cos(theta))*rhat*rhat.T
R3 = sin(theta)*Matrix([[0, -p3, p2],[p3, 0, -p1],[-p2, p1, 0]])
R = R1 + R2 + R3

{% endhighlight %}

Notice how close the syntax is to the mathematical notation! In Sympy, recovering
the Jacobian $$\frac{\partial R}{\partial \theta}$$ is super simple.

{% highlight python %}
R.diff(p1)
R.diff(p2)
R.diff(p3)
{% endhighlight %}

That was the tricky part, now we define the rest of our warping function and do
the same thing.

{% highlight python %}
p4,p5,p6 = symbols('p4 p5 p6', real=True)
x1,x2,x3 = symbols('x1 x2 x3', real=True)

t = Matrix([[p4],[p5],[p6]])
x = Matrix([[x1],[x2],[x3]])

xp = R*x + t
u = xp[0] / xp[2]
v = xp[1] / xp[2]

u.diff(p1)
# etc...
{% endhighlight %}

For fun, let's look at the latex rendering `u.diff(p1)`, which represents one entry in our 12 entry Jacobian:

$$
\frac{x_{1} \left(\frac{p_{1}^{3} \sin{\left (|p| \right )}}{|p|^{3}} -
\frac{2 p_{1}^{3} \left(- \cos{\left (|p| \right )} + 1\right)}{|p|^{4}} -
\frac{p_{1} \sin{\left (|p| \right )}}{|p|} + \frac{2 p_{1} \left(- \cos{\left
(|p| \right )} + 1\right)}{|p|^{2}}\right) \\ + x_{2} \left(\frac{p_{1}^{2} p_{2}
\sin{\left (|p| \right )}}{|p|^{3}} - \frac{2 p_{1}^{2} p_{2} \left(- \cos{\left
(|p| \right )} + 1\right)}{|p|^{4}} - \frac{p_{1} p_{3} \cos{\left (|p| \right
)}}{|p|} + \frac{p_{2} \left(- \cos{\left (|p| \right )} +
1\right)}{|p|^{2}}\right) \\ + x_{3} \left(\frac{p_{1}^{2} p_{3} \sin{\left (|p|
\right )}}{|p|^{3}} - \frac{2 p_{1}^{2} p_{3} \left(- \cos{\left (|p| \right )}
+ 1\right)}{|p|^{4}} + \frac{p_{1} p_{2} \cos{\left (|p| \right )}}{|p|} +
\frac{p_{3} \left(- \cos{\left (|p| \right )} + 1\right)}{|p|^{2}}\right)}{p_{6}
+ x_{1} \left(\frac{p_{1} p_{3} \left(- \cos{\left (|p| \right )} +
1\right)}{|p|^{2}} - p_{2} \sin{\left (|p| \right )}\right) \\ + x_{2} \left(p_{1}
\sin{\left (|p| \right )} + \frac{p_{2} p_{3} \left(- \cos{\left (|p| \right )}
+ 1\right)}{|p|^{2}}\right) \\ + x_{3} \left(\frac{p_{3}^{2} \left(- \cos{\left
(|p| \right )} + 1\right)}{|p|^{2}} + \cos{\left (|p| \right )}\right)} +
\frac{\left(- x_{1} \left(\frac{p_{1}^{2} p_{3} \sin{\left (|p| \right
)}}{|p|^{3}} - \frac{2 p_{1}^{2} p_{3} \left(- \cos{\left (|p| \right )} +
1\right)}{|p|^{4}} - \frac{p_{1} p_{2} \cos{\left (|p| \right )}}{|p|} +
\frac{p_{3} \left(- \cos{\left (|p| \right )} + 1\right)}{|p|^{2}}\right) \\ -
x_{2} \left(\frac{p_{1}^{2} \cos{\left (|p| \right )}}{|p|} + \frac{p_{1} p_{2}
p_{3} \sin{\left (|p| \right )}}{|p|^{3}} - \frac{2 p_{1} p_{2} p_{3} \left(-
\cos{\left (|p| \right )} + 1\right)}{|p|^{4}} + \sin{\left (|p| \right
)}\right) \\ - x_{3} \left(\frac{p_{1} p_{3}^{2} \sin{\left (|p| \right
)}}{|p|^{3}} - \frac{2 p_{1} p_{3}^{2} \left(- \cos{\left (|p| \right )} +
1\right)}{|p|^{4}} - \frac{p_{1} \sin{\left (|p| \right )}}{|p|}\right)\right)
\left(p_{4} + x_{1} \left(\frac{p_{1}^{2} \left(- \cos{\left (|p| \right )} +
1\right)}{|p|^{2}} + \cos{\left (|p| \right )}\right) \\ + x_{2} \left(\frac{p_{1}
p_{2} \left(- \cos{\left (|p| \right )} + 1\right)}{|p|^{2}} - p_{3} \sin{\left
(|p| \right )}\right) \\ + x_{3} \left(\frac{p_{1} p_{3} \left(- \cos{\left (|p|
\right )} + 1\right)}{|p|^{2}} + p_{2} \sin{\left (|p| \right
)}\right)\right)}{\left(p_{6} + x_{1} \left(\frac{p_{1} p_{3} \left(- \cos{\left
(|p| \right )} + 1\right)}{|p|^{2}} - p_{2} \sin{\left (|p| \right )}\right) \\ +
x_{2} \left(p_{1} \sin{\left (|p| \right )} + \frac{p_{2} p_{3} \left(-
\cos{\left (|p| \right )} + 1\right)}{|p|^{2}}\right) \\ + x_{3}
\left(\frac{p_{3}^{2} \left(- \cos{\left (|p| \right )} + 1\right)}{|p|^{2}} +
\cos{\left (|p| \right )}\right)\right)^{2}}
$$

So, this hopefully demonstrates the power of Sympy to solve difficult math
problems for you. In fact, it comes with some tools to clean up the
representation so the fully analytic solution could be exported without too many
repeat calculations of common expressions.

For example, check out the `cse()` function, that groups common expressions and
lists what order they should be evaluated. For the same expression above, we
get:

{% highlight python %}

cse(u.diff(p1),symbols=utilities.iterables.numbered_symbols(prefix='var'),optimizations='basic')

# output:

"""

# variable assignments:

[(var0, p1**2), (var1, p3**2), (var2, p2**2 + var0 + var1), (var3, sqrt(var2)),
(var4, sin(var3)), (var5, p2*var4), (var6, 1/var2), (var7, cos(var3)), (var8,
var7 - 1), (var9, var6*var8), (var10, p3*var9), (var11, p1*var10), (var12,
p1*var4), (var13, -var7), (var14, 1/(p6 - x1*(var11 + var5) + x2*(-p2*var10 +
var12) - x3*(var1*var9 + var13))), (var15, 1/var3), (var16, var15*var7),
(var17, p1*var16), (var18, p2*var17), (var19, p3*var4), (var20, var2**(-3/2)),
(var21, var0*var20), (var22, var19*var21), (var23, 2/var2**2), (var24,
p3*var23*var8), (var25, var15*var4), (var26, var20*var4), (var27, var13 + 1),
(var28, var27*var6), (var29, var23*var27), (var30, var0*var29), (var31, p1*p2)]

# calculation:

-var14*(p1*x1*(-var0*var26 + var25 - 2*var28 + var30) +
var14*(-p1*x3*(-var1*var26 + var1*var29 + var25) - x1*(-p3*var28 + p3*var30 +
var18 - var22) + x2*(p2*p3*var12*var20 + var0*var16 + var24*var31 + var4))*(p4
- x1*(var0*var9 + var13) - x2*(var19 + var31*var9) - x3*(var11 - var5)) +
x2*(-p2*var28 + p2*var30 + p3*var17 - var21*var5) - x3*(var0*var24 - var10 +
var18 + var22))
"""
{% endhighlight %}

We get a list of tuples with suggested variable names and what their value
should be. Using these variables, we perform the proper calculation while
trying to minimize common expression evaluation. Since Sympy uses `xN` style
variable naming by default, I set the `symbols` to a new iterator `varN` so they
can be told apart.

This also suggests some approximations that could reduce further the number of
add-multiplies required to compute our result. It's interesting to look at the
structure of this expression, the calculations involving $$\theta$$ (or `p` in
our Sympy code) only need to be calculated once, and can be reused over every
pixel in the image. In particular, our CSE expression only depends on `x` values
for `var14` which has no later dependencies in the variable assignment step.
Thus, each pixel only needs to assign `var14` when given the other variables, and
the full expression contains few add/multiplies. I was surprised to find that
all the weird operations (e.g. `sin`/`cos` and `sqrt`) are isolated to the variable
assignment, which only needs to happen once per optimization step (rather than
for every pixel). This suggests the potential for silly fast implementations
using a GPU, or even a fairly straightforward architecture to implement on an
FPGA, if we want to go plaid...

To make it even simpler, Sympy has a `lambdify` mode, that allows you to convert
a symbolic expression into something that can be evaluated using NumPy or other
libraries as a backend.

*Note [4/18/2019]: As explained above, there is much more I wanted to cover. I even promised to discuss feature point methods in Part 1. Hopefully one day I can update this post!*
